{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever And Chain With Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token loaded: True\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 1: Imports & Environment\n",
    "# ================================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load env file (.env should have HF_TOKEN=your_token)\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "print(\"Hugging Face token loaded:\", bool(hf_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents/pages: 15\n",
      "First document preview:\n",
      " Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Cell 2: Load Documents\n",
    "# ================================\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"attention.pdf\")  # replace with your PDF\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents/pages: {len(docs)}\")\n",
    "print(\"First document preview:\\n\", docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 93\n",
      "First chunk preview:\n",
      " Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,   # smaller to avoid token limit errors\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"First chunk preview:\\n\", chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 93\n",
      "First chunk preview:\n",
      " Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Cell 3: Split into Chunks\n",
    "# ================================\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,   # keep smaller to avoid sequence length issues\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(\"First chunk preview:\\n\", chunks[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face LLM initialized\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"token\": hf_token})\n",
    "print(\"Hugging Face LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created with Hugging Face embeddings\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 4: Embeddings & Vector Store\n",
    "# ================================\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "print(\"FAISS vector store created with Hugging Face embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 5: LLM Setup\n",
    "# ================================\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# Use a small, free, local model\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"Hugging Face LLM initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template ready\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 6: Prompt Template\n",
    "# ================================\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "If the context does not contain the answer, say \"I don’t know based on the provided context.\"\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "print(\"Prompt template ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval chain ready\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 7: Document Chain + Retrieval Chain\n",
    "# ================================\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "print(\"Retrieval chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n",
      "a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. So the final answer is a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is transformer architecture?\"\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"Query result:\")\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is transformer architecture?\n",
      "A: The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 8: Test Queries\n",
    "# ================================\n",
    "query = \"What is transformer architecture?\"\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "print(\"Q:\", query)\n",
    "print(\"A:\", response['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who are the authors of Attention is All You Need?\n",
      "A: [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen [16] ukasz Kaiser and Samy Bengio.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who are the authors of Attention is All You Need?\"\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "print(\"Q:\", query)\n",
    "print(\"A:\", response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is duck?\n",
      "A: I don’t know based on the given context.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is duck?\"  # random/unrelated question\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "print(\"Q:\", query)\n",
    "print(\"A:\", response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
